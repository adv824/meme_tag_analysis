{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import string\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "from gensim.models import Doc2Vec\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import multiprocessing\n",
    "cores = multiprocessing.cpu_count()\n",
    "\n",
    "from sklearn import utils\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('train_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12041"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Drop nas in about\n",
    "\n",
    "data.dropna(subset=['about'],inplace=True)\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Convert the text in 'about' to lowercase, remove punctuation\n",
    "\n",
    "def cleanText(text):\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    text = text.lower()\n",
    "    return text\n",
    "\n",
    "data['clean_about'] = data['about'].apply(cleanText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create four equal sized buckets to be used as the predictive variable and add it to the dataframe\n",
    "\n",
    "meme_popularity = pd.cut(data['target_variable'],\n",
    "                    [0,data['target_variable'].quantile(0.25),data['target_variable'].quantile(0.5),data['target_variable'].quantile(0.75),max(data['target_variable'])],\n",
    "                   labels = ['low','medium','high','great'])\n",
    "\n",
    "data['popularity'] = meme_popularity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tokenize the clean text and remove stopwords\n",
    "\n",
    "def tokenize_text(text):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = []\n",
    "    for sent in nltk.sent_tokenize(text):\n",
    "        for word in nltk.word_tokenize(sent):\n",
    "            if len(word) < 2:\n",
    "                continue\n",
    "            if word not in stop_words:\n",
    "                tokens.append(word.lower())\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tag the tokenized document with the populariity associated with each document\n",
    "\n",
    "about_tagged = data.apply(\n",
    "    lambda t: TaggedDocument(words=tokenize_text(t['clean_about']), tags=t.popularity), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12041/12041 [00:00<00:00, 2700294.84it/s]\n"
     ]
    }
   ],
   "source": [
    "## Build a Distributed Bag-of-words model and build the vocab\n",
    "\n",
    "model_dbow = Doc2Vec(dm=0, vector_size=15, negative=5, hs=0, min_count=2, sample = 0, workers=cores)\n",
    "model_dbow.build_vocab([x for x in tqdm(about_tagged.values)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12041/12041 [00:00<00:00, 2605696.75it/s]\n",
      "100%|██████████| 12041/12041 [00:00<00:00, 3569664.58it/s]\n",
      "100%|██████████| 12041/12041 [00:00<00:00, 3688010.40it/s]\n",
      "100%|██████████| 12041/12041 [00:00<00:00, 3322606.21it/s]\n",
      "100%|██████████| 12041/12041 [00:00<00:00, 3539394.10it/s]\n",
      "100%|██████████| 12041/12041 [00:00<00:00, 3442176.56it/s]\n",
      "100%|██████████| 12041/12041 [00:00<00:00, 3490952.82it/s]\n",
      "100%|██████████| 12041/12041 [00:00<00:00, 2995114.13it/s]\n",
      "100%|██████████| 12041/12041 [00:00<00:00, 3131424.51it/s]\n",
      "100%|██████████| 12041/12041 [00:00<00:00, 3859067.35it/s]\n",
      "100%|██████████| 12041/12041 [00:00<00:00, 3904113.67it/s]\n",
      "100%|██████████| 12041/12041 [00:00<00:00, 3550841.21it/s]\n",
      "100%|██████████| 12041/12041 [00:00<00:00, 3397485.00it/s]\n",
      "100%|██████████| 12041/12041 [00:00<00:00, 3329835.46it/s]\n",
      "100%|██████████| 12041/12041 [00:00<00:00, 3690166.19it/s]\n",
      "100%|██████████| 12041/12041 [00:00<00:00, 3496995.88it/s]\n",
      "100%|██████████| 12041/12041 [00:00<00:00, 3013162.37it/s]\n",
      "100%|██████████| 12041/12041 [00:00<00:00, 3682363.43it/s]\n",
      "100%|██████████| 12041/12041 [00:00<00:00, 3455364.97it/s]\n",
      "100%|██████████| 12041/12041 [00:00<00:00, 3518680.03it/s]\n",
      "100%|██████████| 12041/12041 [00:00<00:00, 2720513.60it/s]\n",
      "100%|██████████| 12041/12041 [00:00<00:00, 2875732.52it/s]\n",
      "100%|██████████| 12041/12041 [00:00<00:00, 3648577.84it/s]\n",
      "100%|██████████| 12041/12041 [00:00<00:00, 3160426.44it/s]\n",
      "100%|██████████| 12041/12041 [00:00<00:00, 3540138.40it/s]\n",
      "100%|██████████| 12041/12041 [00:00<00:00, 3487096.21it/s]\n",
      "100%|██████████| 12041/12041 [00:00<00:00, 3497238.04it/s]\n",
      "100%|██████████| 12041/12041 [00:00<00:00, 3625528.68it/s]\n",
      "100%|██████████| 12041/12041 [00:00<00:00, 3626570.05it/s]\n",
      "100%|██████████| 12041/12041 [00:00<00:00, 3569916.91it/s]\n"
     ]
    }
   ],
   "source": [
    "## Train the model we built above for 30 epochs\n",
    "\n",
    "for epoch in range(30):\n",
    "    model_dbow.train(utils.shuffle([x for x in tqdm(about_tagged.values)]), total_examples=len(about_tagged.values), epochs=1)\n",
    "    model_dbow.alpha -= 0.002\n",
    "    model_dbow.min_alpha = model_dbow.alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Build the Final Vector feature we can use for classification\n",
    "\n",
    "def vec_for_learning(model, tagged_docs):\n",
    "    sents = tagged_docs.values\n",
    "    targets, regressors = zip(*[(doc.tags[0], model.infer_vector(doc.words, steps=20)) for doc in sents])\n",
    "    return targets, regressors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train, X_train = vec_for_learning(model_dbow, about_tagged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/advaitn/.conda/envs/exp2/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Users/advaitn/.conda/envs/exp2/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "## Let's try it out using a simple LogisticRegression\n",
    "\n",
    "logreg = LogisticRegression(n_jobs=1, C=1e5)\n",
    "logreg.fit(X_train, y_train)\n",
    "y_pred = logreg.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.33385931401046426"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Testing the accuracy on training set\n",
    "\n",
    "accuracy_score(y_train, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.83649707, -1.2550204 ,  0.6318553 , -3.3285842 , -1.0448722 ,\n",
       "       -3.2441492 ,  0.81032354, -2.7630506 ,  0.7470689 , -2.5316715 ,\n",
       "       -1.3190255 ,  1.8444138 ,  1.0105989 , -0.20433724, -3.5171907 ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Take a look at the embedding\n",
    "\n",
    "X_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Each document has been converted to a vector of length 15 \n",
    "\n",
    "### X_train contains arrays of tuples each tuple consists of the embedding of the doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
